


<!DOCTYPE html>
<html lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <!-- responsive-->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Archives: 2018 [ Bingo ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="/css/elenore/css/elenore.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    
      <link rel="stylesheet" href="/css/floekr.css">
    
  

  <!-- highlight -->
  <link rel="stylesheet" href="/highlight/styles/monokai-sublime.css">
  <script src="/highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Navigation Bar -->
<nav id="navbar" class="navbar is-white is-fixed-top">
  <div id="specialShadow" class="el-special-shadow"></div>
  <div class="container">

    <!-- left side, always visible -->
    <div class="navbar-brand">
      <a class="navbar-item" href="/">
        <!-- <strong>Bingo</strong> -->
        <i class="fa fa-lg fa-home"></i>
      </a>

      <!-- toggles the menu on touch devices -->
      <div id="navbarBurger" class="navbar-burger burger" data-target="navMenu">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>

    <!-- menu -->
    <div id="navMenu" class="navbar-menu">
        <div class="navbar-end">
          
            
            <a href="/" class="navbar-item">HOME</a>
            

            

            
          
            

            

            
            <a href="/projects" class="navbar-item" >PROJECTS</a>
            
          
            

            
            <a href="/about" class="navbar-item">ABOUT</a>
            

            
          
        </div>
    </div>
  </div>
</nav>

<!-- index hero -->
<div class="hero is-large has-background-fixed" id="indexHero">
  <div class="hero-body">
    <div class="container has-text-centered">
      <p id="titleContent">
        Bingo, <span>Computer Graphics</span> & <span>Game Developer</span>
      </p>
    </div>
  </div>
</div>

<!-- articles and about me -->

<section class="section">
  <div class="container">
    <!-- content and about me -->
    <div class="columns">
      <!-- blog -->
      <div class="column is-two-thirds">
        <div class="columns is-multiline">
          <div class="column is-12">
            <!-- articles -->
            
                
                  <div class="card is-white is-hover" id="articleCard">
                    <div class="card-header">
                      <a href="/2018/03/12/2016-2-21-Camera Model/" class="card-header-title">
                        Camera Model
                      </a>
                    </div>
                    <div class="card-content">
                      <p id="articleContent">
                        <h3 id="Pinhole-Camera-Model"><a href="#Pinhole-Camera-Model" class="headerlink" title="Pinhole Camera Model"></a>Pinhole Camera Model</h3><blockquote>
<p>本文为在<a href="http://www.scratchapixel.com/lessons/3d-basic-rendering/3d-viewing-pinhole-camera" target="_blank" rel="noopener">Scratchapixel</a>上学习相机模型时的个人理解。本文不做翻译或搬运工作，只描述个人学习上的理解。</p>
</blockquote>
<blockquote>
<p>PS: 这一章略过Depth of field(景深)的概念</p>
</blockquote>
<p>易混淆概念解读</p>
<table>
<thead>
<tr>
<th>Camera Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Focal Length</td>
<td>eye到真实film平面的距离，与光圈一道用于计算FOV/AVO。焦距容易和虚拟相机中的虚拟film平面到eye之间的距离混淆，虚拟film平面一般位于近裁剪面</td>
</tr>
<tr>
<td>Camera Aperture</td>
<td>光圈定义了真实相机的物理维度，与焦距一道用于计算FOV/AVO。同时光圈的两个维度也定义了film gate aspect ratio大小。<a href="https://en.wikipedia.org/wiki/List_of_film_formats" target="_blank" rel="noopener">Wiki</a>中列举了大多数常见的film参数。</td>
</tr>
<tr>
<td>Clipping Planes</td>
<td>远近裁剪面是虚拟的平面，其位于摄像机的视锥体中，只有在远近裁剪面中的对象才会被渲染。由于画布经常与近裁剪面放置在一起，因此要提防与Focal Length概念混淆。</td>
</tr>
<tr>
<td>Image Size</td>
<td>输出图像的尺寸/像素，图像尺寸定义了resolution gate aspect ratio。</td>
</tr>
</tbody>
</table>
<p>由以上的这些概念可以推导计算出下面这些变量的值</p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Decription</th>
</tr>
</thead>
<tbody>
<tr>
<td>Angle of View</td>
<td>由Focal Length与Film Size(Camera Aperture)计算得到</td>
</tr>
<tr>
<td>Canvas/Screen Window</td>
<td>其宽高比例与真实光圈定义得到的film gate aspect ratio一致，可以根据Canvas Size以及光圈快门宽高比例计算得到</td>
</tr>
<tr>
<td>Film Gate Aspect Ratio</td>
<td>$\frac{\text{film width}}{\text{film height}}$，这里film为真实相机的胶卷尺寸</td>
</tr>
<tr>
<td>Resolution Gate Aspect Ratio</td>
<td>$\frac{\text{image width}}{\text{image height}}$，这里image为输出的像素宽高比图像的</td>
</tr>
</tbody>
</table>
<p>光圈(Aperature):本质上，在平红相机模型中，光圈就是暗室上的那个小洞。但是在现实生活中光圈(pin hole camera model)有一系列的问题。</p>
<p>首先，较大的光圈将会产生模糊的图像(在物体上的同一个位置反射的光子有较大概率在背景布上重叠)，但可以大大增大进光量减少曝光时间(曝光时间越长，那么拍摄非静止物体就会模糊)。</p>
<p>然而，为了得到一个边缘清晰锐利可辨的图像，减小光圈直径又是必要的，但这也要求曝光时间增加来提高画面亮度，这也提高了画面模糊的概率。</p>
<blockquote>
<p>上述概念可由下两动图解释</p>
</blockquote>
<figure><br>    <a href="http://www.scratchapixel.com/images/upload/cameras/pinhole1.gif" target="_blank" rel="noopener"><br>        <img src="http://www.scratchapixel.com/images/upload/cameras/pinhole1.gif" alt=""><br>    </a><br></figure>

<figure><br>    <a href="http://www.scratchapixel.com/images/upload/cameras/pinhole2.gif" target="_blank" rel="noopener"><br>        <img src="http://www.scratchapixel.com/images/upload/cameras/pinhole2.gif" alt=""><br>    </a><br></figure>


<p>为了解决曝光时间和边缘锋利的矛盾，人们将平红相机的洞替换为了<code>凸透镜</code>。如图所示，更大的光圈可以带来更佳的进光量，减少曝光时间，同时，他也可以将一定距离内的物体上反射的光重新在背景布上汇聚(不同的焦距将会带来不同的景深)。</p>
<blockquote>
<p>景深表示在场景中，可见的最远的物体和最近的物体之间的距离(边缘锋利可辨)。因此平红相机的景深为无限大，因为他只是简单的讲光路重现在背景布上，不存在透镜相机的光路汇集的情况。</p>
</blockquote>
<figure><br>    <a href="http://www.scratchapixel.com/images/upload/cameras/lens.png" target="_blank" rel="noopener"><br>        <img src="http://www.scratchapixel.com/images/upload/cameras/lens.png" alt=""><br>    </a><br></figure>

<p>视角(Angle of View/AOV, Field of View/FOV):视角与两个参数息息相关，底片尺寸与焦距。也就是说改变两者中的任何一个都会引起视角的变化。</p>
<figure><br>    <a href="http://www.scratchapixel.com/images/upload/cameras/filmsize3.png" target="_blank" rel="noopener"><br>        <img src="http://www.scratchapixel.com/images/upload/cameras/filmsize3.png" alt=""><br>    </a><br></figure><br>&gt; 相同的焦距不同的底片大小将会导致最终图像的内容变少<br><br><figure><br>    <a href="http://www.scratchapixel.com/images/upload/cameras/filmsize4.png" target="_blank" rel="noopener"><br>        <img src="http://www.scratchapixel.com/images/upload/cameras/filmsize4.png" alt=""><br>    </a><br></figure>

<blockquote>
<p>同样的若想要在不同的底片大小上获得一致的内容，那么就要适当的调整焦距大小，这也就带来清晰度的影响。因为底片越大，相同的内容展示的也就越清晰。</p>
</blockquote>
<blockquote>
<p>同时这三个参数都是互相衔接的，也就是只要知道三者中的任何两个，就可以计算出剩下一个参数的大小。当然数字相机拍摄的画面已经不再受到底片的大小的影响了(传感器大小)。</p>
</blockquote>
<p>胶片框比例(File Gate Radio)与图像分辨率比例(Resolution Gate Ratio):这两者是很有可能不一致的，那么这时候你使用的相机就是变形镜头。 </p>
<blockquote>
<p>两者比例不同时，你可能面临画面选择的问题。因此Maya给出两个选项:过扫描(Oversan)和填充(Fill)如下图。</p>
</blockquote>
<figure><br>    <a href="http://www.scratchapixel.com/images/upload/cameras/filmgate3.png" target="_blank" rel="noopener"><br>        <img src="http://www.scratchapixel.com/images/upload/cameras/filmgate3.png" alt=""><br>    </a><br></figure>

<p>基本未知量推导</p>
<figure><br>    <a href="http://www.scratchapixel.com/images/upload/cameras/canvascoordinates4.png" target="_blank" rel="noopener"><br>        <img src="http://www.scratchapixel.com/images/upload/cameras/canvascoordinates4.png" alt=""><br>    </a><br></figure>

<p>相机原点就可以理解为Eye所在位置</p>
<p>$$\begin{array}{l}<br>\tan({\theta_H \over 2}) &amp; = &amp; {A \over B} \\&amp; = &amp; \color{red}{\dfrac {\dfrac { (\text{Film Aperture Width} * 25.4) } { 2 } } { \text{Focal Length} }}.<br>\end{array}$$</p>
<blockquote>
<p>$\theta$是FOV(视角/视野)，可以通过$arctan(\frac{A}{B})$求解显示得到。</p>
</blockquote>
<blockquote>
<p>$Canvas Size$也就是所谓的近裁剪面，因此近裁剪面的位置不同，画布的大小也会随之改变。当近裁剪面位于$z = 0$位置时，相应计算而来的$Canvas Size = 0$</p>
</blockquote>
<p>$$\begin{array}{l}<br>\tan({\theta_H \over 2}) = {A \over B} =<br>\dfrac{\dfrac{\text{Canvas Width} } { 2 } } { Z_{near} }, \\<br>\dfrac{\text{Canvas Width} } { 2 } = \tan({\theta_H \over 2}) <em> Z_{near},\\<br>\text{Canvas Width}= 2 </em> \color{red}{\tan({\theta_H \over 2})} * Z_{near}.<br>\end{array}$$</p>
<blockquote>
<p>只要得到Canvas Size就可以通过aspect ratio以及简单的坐标变换得到[left, top, right, bottom]各自的值。</p>
</blockquote>
<p>$$\begin{array}{l}<br>\text{right} = \color{red}{\dfrac {\dfrac { (\text{Film Aperture Width} <em> 25.4) } { 2 } } { \text{Focal Length} }} </em> Z_{near}.<br>\end{array}$$</p>
<p>$$\text{top} = \color{red}{\dfrac {\dfrac { (\text{Film Aperture Height} <em> 25.4) } { 2 } } { \text{Focal Length} }} </em> Z_{near}.$$</p>
<hr>
<h3 id="PBRT中的相机坐标系变换"><a href="#PBRT中的相机坐标系变换" class="headerlink" title="PBRT中的相机坐标系变换"></a>PBRT中的相机坐标系变换</h3><ul>
<li>Camera Space: 以相机的origin作为坐标轴的中心，z轴作为视线方向，y轴作为抬头方向(up direction)，用于判断一个物体是否对于相机本身是否可见会非常的方便。</li>
<li>Screen Space: 屏幕空间本身是在图像面板上定义出的，尽管被称之为屏幕空间，但z值依然是有意义的(相关于远近裁剪面，其范围为$[0, 1]$)。</li>
<li>Normalized device coordinate(NDC) space : 这是一个真实图像被渲染的坐标系，xy的范围都在$[0, 1]$之间，z值和屏幕空间坐标系一样有效，本质上也就是屏幕空间线性转换而来。</li>
<li>Raster Space: 本质上他和NDC Space并没有太大的差别，唯独的区别就是他的xy范围在$[0, xResolution]$和$[0, yResolution]$</li>
</ul>

                      </p>
                    </div>
                  </div>
                  <br>
                  <br>
                  
                  <div class="card is-white is-hover" id="articleCard">
                    <div class="card-header">
                      <a href="/2018/03/12/2016-2-05-Mathematical Foundations of Monte Carlo Methods 4/" class="card-header-title">
                        Mathematical Foundations of Monte Carlo Methods 4
                      </a>
                    </div>
                    <div class="card-content">
                      <p id="articleContent">
                        <blockquote>
<p>本文为在<a href="http://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-mathematical-foundations/quick-introduction-to-monte-carlo-methods" target="_blank" rel="noopener">Scratchapixel</a>上学习的翻译读后感与部分个人解读。这里不会将全篇的内容系数翻译，保留原文以便后期自行理解，笔者只精炼一些文章中关键的点出来便于记录。</p>
</blockquote>
<h3 id="Hit-or-Miss-Monte-Carlo-Method"><a href="#Hit-or-Miss-Monte-Carlo-Method" class="headerlink" title="Hit-or-Miss Monte Carlo Method"></a>Hit-or-Miss Monte Carlo Method</h3><p>蒙特卡洛方法(Monte Carlo methods):是一个使用随机采样的数值方法来解决数学问题的方法。</p>
<blockquote>
<p>原始的蒙特卡洛方法是允许任意分布采样的，所有的非均匀采样的目的都是降低方差提高估计量的有效性也就是重要性采样。</p>
</blockquote>
<p>各向同性散射(Isotropic scattering)与各向异性散射(Anistropic scattering):一个光子在进入一个材质时发生了散射，并且其改变后了的方向是随机的，被称为是各项同性；反之，光子改变后了的方向若是只在一个圆锥方向内，那么被称之为各向异性。</p>
<hr>
<h3 id="Monte-Carlo-Estimator"><a href="#Monte-Carlo-Estimator" class="headerlink" title="Monte Carlo Estimator"></a>Monte Carlo Estimator</h3><p>对于函数$f(x)$的积分求解，可以用面积法来表达</p>
<p>$$F = \int_a^b f(x)\;dx.$$</p>
<p>下图中，在函数$f(x)$上随机采样一点$x$，那么结果$f(x) * (b-a)$。但是很明显它和实际收敛结果$F$差距较大。</p>
<figure><br><br><a href="https://farm2.staticflickr.com/1495/24806240436_aa8e259d2f.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1495/24806240436_aa8e259d2f.jpg" alt=""><br></a><br><br></figure>

<p>增加采样数目到4个点，此时对这几个点所求面积进行平均。那么最终的近似结果很明显根据大数定理会不断的逼近真实收敛值。</p>
<figure><br><br><a href="https://farm2.staticflickr.com/1718/24204329434_e58e2aa085.jpgg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1718/24204329434_e58e2aa085.jpg" alt=""><br></a><br><br></figure>

<p>以下公式很好的表达了这个思想</p>
<p>$$\langle F^N\rangle = (b-a) \dfrac{1}{N } \sum_{i=0}^{N-1} f(X_i).$$</p>
<blockquote>
<p>其中$\langle F^N\rangle$表示了在采样空间$S$中采样N个点之后$F$的近似值，也等价于之前所讲到的样本均值$\bar X_n$。</p>
<p>这里采样点满足均匀分布，即$pdf(x) = \frac{1}{b-a}$。</p>
</blockquote>
<p>$\langle F^N \rangle$也是随机变量(随机变量的和)，其期望就是$F$本身。</p>
<p>$$\begin{array}{l}</p>
<p>E[\langle F^N \rangle] &amp; = &amp; E \left[ (b-a) \dfrac{1}{N } \sum_{i=0}^{N-1} f(x_i)\right],\\</p>
<p>&amp; = &amp; (b-a)\dfrac{1}{N}\sum_{i=0}^{N-1}E[f(x)],\\</p>
<p>&amp; = &amp;(b-a)\dfrac{1}{N} \sum_{i=0}^{N-1} \int_a^b f(x)pdf(x)\:dx\\</p>
<p>&amp; = &amp; \dfrac{1}{N} \sum_{i=0}^{N-1} \int_a^b f(x)\:dx,\\</p>
<p>&amp;=&amp; \int_a^b f(x)\:dx,\\</p>
<p>&amp;=&amp;F\\</p>
<p>\end{array}$$</p>
<blockquote>
<p>原文中公式有误，已纠正求和部分</p>
</blockquote>
<p>由于前面选用了均匀分布的$pdf(x)$的缘故，下面推广到任意$pdf(x)$上。下方为通用的蒙特卡洛估计量的写法。</p>
<p>$$\langle F^N \rangle = \dfrac{1}{N} \sum_{i=0}^{N-1} \dfrac{f(X_i)}{pdf(X_i)}.$$</p>
<p>对其求取期望验证正确性</p>
<p>$$\begin{array}{l}</p>
<p>E[\langle F^N \rangle ] &amp; = &amp; E \left[ \dfrac{1}{N } \sum_{i=0}^{N-1} \dfrac{f(X_i)}{pdf X_i)} \right],\ </p>
<p>&amp; = &amp; \dfrac{1}{N} \sum_{i=0}^{N-1} E\left[ \dfrac{f(X_i)}{pdf(X_i) }\right],\ </p>
<p>&amp; = &amp; \dfrac{1}{N} \sum_{i=0}^{N-1} \int_\Omega \dfrac{f(x)}{pdf(x)} pdf(x)\;dx, \\</p>
<p>&amp; = &amp; \dfrac{1}{N} \sum_{i=0}^{N-1} \int_\omega f(x) \; dx, \ </p>
<p>&amp; = &amp; F.</p>
<p>\end{array}$$</p>
<blockquote>
<p>上下限$(b-a)$这样一个积分区间在通用写法中是隐藏的。原因很简单，$(b-a)$的产生是因为估计量$h(x) = \frac{f(x)}{pdf(x)}$下均匀分布的$pdf(x) = \frac{1}{b-a}$引起的。</p>
<p>因此原式应该是这样:$\langle F^N \rangle  = \frac{1}{N}\sum_{i=0}^{N-1}\frac{f(X_i)}{\frac{1}{b-a}}$，而写法$\langle F^N\rangle = (b-a) \dfrac{1}{N } \sum_{i=0}^{N-1} f(X_i)$会更容易能够从图中直接推出。</p>
</blockquote>
<hr>
<h3 id="Properties-of-Monte-Carlo-Integration"><a href="#Properties-of-Monte-Carlo-Integration" class="headerlink" title="Properties of Monte Carlo Integration"></a>Properties of Monte Carlo Integration</h3><ul>
<li>蒙特卡洛积分估计值会向函数$f(x)$收敛/逼近。$\text{Pr} \left ( \lim_{N\to\infty} \langle F^N \rangle = F \right ) = 1$</li>
<li>蒙特卡罗估计量是无偏且一致的</li>
<li>收敛的速度和函数的方差$\sigma^2$成比例。估计量本身的方差为$\frac{\sigma^2}{n}$，因此如若需要降低估计值错误为原来的一半，那么需要提高四倍的采样。($\sigma[\langle F^N \rangle] \propto { 1 \over \sqrt{N} }$)</li>
</ul>
<blockquote>
<p>无偏:样本均值的期望就是求解积分本身</p>
<p>一致:随着样本容量的增大，估计量愈来愈接近总体参数的真值)</p>
</blockquote>
<hr>
<h3 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h3><p>重要性采样作为减小方差众多方法中的一个，本身的思想较为直接。</p>
<p>以下为不同采样分布的采样点对于近似值的影响(会高于或者低于真实积分解)。</p>
<figure><br><br><a href="https://farm2.staticflickr.com/1662/24807155306_369d329969.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1662/24807155306_369d329969.jpg" alt=""><br></a><br><br></figure>

<p>此图中均匀分布其值勉强，但采样过程中似乎遗漏了函数当中较为重要的部分(一个高峰被忽略)。而右边的人为的采样也并不是一个较好的方法，这将会导致偏差(bias)。</p>
<figure><br><br><a href="https://farm2.staticflickr.com/1496/24537947180_f7e1e82d22.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1496/24537947180_f7e1e82d22.jpg" alt=""><br></a><br><br></figure>

<p>如若被积函数为常数函数，那么采样选用均匀分布得到的结果本身就是正确的。</p>
<figure><br><br><a href="https://farm2.staticflickr.com/1608/24465788229_7c5beb558a.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1608/24465788229_7c5beb558a.jpg" alt=""><br></a><br><br></figure>

<p>现有一函数，他与函数$f(x)$成比例</p>
<p>$$f(x) = c f’(x)$$</p>
<p>因此</p>
<p>$$\dfrac{f(x)}{f’(x) } = \dfrac{1}{c}$$</p>
<p>那么这里代入蒙特卡洛估计量，联系常数函数的采样的结论。</p>
<p>$$\langle F^N \rangle = \dfrac{1}{N} \sum_{i=0}^{N-1} {\dfrac{\color{orange}{f(x)}}{\color{red}{pdf(x)}}} = \dfrac{1}{N}\sum_{i=0}^{N-1}{\dfrac{\color{orange}{f(x)}}{\color{red}{f’(x)}}} = \dfrac{1}{N}\sum_{i=0}^{N-1}{\dfrac{\color{orange}{1}}{\color{red}{c}}}$$</p>
<blockquote>
<p>也就是说，只要$pdf(x)$与被积函数$f(x)$成比例，蒙特卡洛积分的方差就是0(常数函数的方差为0)。换言之，$pdf(x)$与被积函数$f(x)$的相似度越高，那么偏差也就越低。</p>
</blockquote>
<p>以$f(x) = sin(x)$，区间$[0, \frac{\pi}{2}]$为例</p>
<p>$$\begin{array}{l}</p>
<p>F &amp; = &amp; \int_0^{\pi \over 2} \sin(x) \; dx \\</p>
<p>&amp; = &amp; \left[ -\cos(x) \right]_0^{\pi \over 2} \\</p>
<p>&amp; = &amp; -\cos(\dfrac{\pi}{2}) - - \cos(0) \\</p>
<p>&amp; = &amp; 1.</p>
<p>\end{array}$$</p>
<p>选用两个不同的$pdf(x)$进行对比</p>
<figure><br><br><a href="https://farm2.staticflickr.com/1507/24206886533_23cf0b4feb.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1507/24206886533_23cf0b4feb.jpg" alt=""><br></a><br><br></figure>

<table>
<thead>
<tr>
<th>#</th>
<th style="text-align:center">Uniform</th>
<th style="text-align:center">Importance</th>
<th style="text-align:center">Error Uniform %</th>
<th style="text-align:center">Error Importance %</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td style="text-align:center">1.125890</td>
<td style="text-align:center">0.969068</td>
<td style="text-align:center">12%</td>
<td style="text-align:center">-3%</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center">1.277833</td>
<td style="text-align:center">0.925675</td>
<td style="text-align:center">27%</td>
<td style="text-align:center">-7%</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center">1.054394</td>
<td style="text-align:center">0.980940</td>
<td style="text-align:center">5%</td>
<td style="text-align:center">-1%</td>
</tr>
<tr>
<td>3</td>
<td style="text-align:center">1.125890</td>
<td style="text-align:center">0.969068</td>
<td style="text-align:center">12%</td>
<td style="text-align:center">-1%</td>
</tr>
<tr>
<td>4</td>
<td style="text-align:center">1.125890</td>
<td style="text-align:center">0.969068</td>
<td style="text-align:center">12%</td>
<td style="text-align:center">-6%</td>
</tr>
<tr>
<td>5</td>
<td style="text-align:center">0.830151</td>
<td style="text-align:center">1.041751</td>
<td style="text-align:center">-16%</td>
<td style="text-align:center">4%</td>
</tr>
<tr>
<td>6</td>
<td style="text-align:center">1.062268</td>
<td style="text-align:center">0.989363</td>
<td style="text-align:center">6%</td>
<td style="text-align:center">-1%</td>
</tr>
<tr>
<td>7</td>
<td style="text-align:center">0.849265</td>
<td style="text-align:center">1.043809</td>
<td style="text-align:center">-15%</td>
<td style="text-align:center">4%</td>
</tr>
<tr>
<td>8</td>
<td style="text-align:center">0.921527</td>
<td style="text-align:center">1.020279</td>
<td style="text-align:center">-7%</td>
<td style="text-align:center">2%</td>
</tr>
<tr>
<td>9</td>
<td style="text-align:center">1.002310</td>
<td style="text-align:center">0.994284</td>
<td style="text-align:center">0%</td>
<td style="text-align:center">0%</td>
</tr>
</tbody>
</table>
<p>很明显的是，结果非常符合重要性采样理论。</p>
<hr>
<h3 id="Quasi-Monte-Carlo"><a href="#Quasi-Monte-Carlo" class="headerlink" title="Quasi Monte Carlo"></a>Quasi Monte Carlo</h3><p>随机采样中无法避免的就是当采样点近乎重合的现象(clump)，这也就意味着在最终计算时，其中一个采样点的信息也就被浪费，这不利于收敛的快速计算。</p>
<figure><br><br><a href="https://farm2.staticflickr.com/1471/24538477960_7d0c21d3f9.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1471/24538477960_7d0c21d3f9.jpg" alt=""><br></a><br><br></figure>

<p><br></p>
<p>分层采样(Stratified Sampling): The interval of integration is divided into N subintervals or cells (also often called strata), samples are placed in the middle of these subintervals but are jittered by some negative or positive random offset which can’t be greater than half the width of a cell</p>
<figure><br><br><a href="https://farm2.staticflickr.com/1596/24716059352_d96be6a9f2.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1596/24716059352_d96be6a9f2.jpg" alt=""><br></a><br><br></figure>


<blockquote>
<p>换言之就是$\langle F^N \rangle = { (b-a) \over N} \sum_{i=0}^{N-1} f(a + ( { {i+\xi} \over N } ) (b-a)).$，其中$-h/2 \leq \xi \leq h/2$。分层采样的思想介于随机采样和均匀采样之间的。</p>
</blockquote>
<p>低差异化序列(Low-Discrepancy Sequences):The goal is to generate sequences of samples which are not exactly uniformly distributed (uniformly distributed samples cause aliasing) and yet which appear to have some regularity in the way they are spaced.</p>
<blockquote>
<p><a href="http://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/introduction-quasi-monte-carlo" target="_blank" rel="noopener">Van der Corput Sequence</a>的简介可见链接，思想就是将整数转换为二进制形式，根据小数点镜像对称。根据$\phi_b(n) = {d_0 \over {2^1}} + {d_1 \over {2^2}} … + {d_{N-1}\over {2^N}}.$将给定的整数转换为小数形式。</p>
</blockquote>
<figure><br><br><a href="https://farm2.staticflickr.com/1597/24205854704_6ec6ec0353.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1597/24205854704_6ec6ec0353.jpg" alt=""><br></a><br><br></figure>
                      </p>
                    </div>
                  </div>
                  <br>
                  <br>
                  
                  <div class="card is-white is-hover" id="articleCard">
                    <div class="card-header">
                      <a href="/2018/03/12/2016-1-28-Mathematical Foundations of Monte Carlo Methods 3/" class="card-header-title">
                        Mathematical Foundations of Monte Carlo Methods 3
                      </a>
                    </div>
                    <div class="card-content">
                      <p id="articleContent">
                        <blockquote>
<p>本文为在<a href="http://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-mathematical-foundations/quick-introduction-to-monte-carlo-methods" target="_blank" rel="noopener">Scratchapixel</a>上学习的翻译读后感与部分个人解读。这里不会将全篇的内容系数翻译，保留原文以便后期自行理解，笔者只精炼一些文章中关键的点出来便于记录。</p>
</blockquote>
<h3 id="The-Probability-Distribution-Function"><a href="#The-Probability-Distribution-Function" class="headerlink" title="The Probability Distribution Function"></a>The Probability Distribution Function</h3><p>概率密度函数(Probabilify density function):When a function such as the normal distribution defines a continuous probability distribution. In other words, pdfs are used for continuous random variables.</p>
<p>The PDF can be used to calculate the probability that a random variable lies within an interval:</p>
<p>$$Pr(a \leqslant X \leqslant b) = \int^b_a pdf(x)dx$$</p>
<p>概率密度函数对概率的积分必为1</p>
<p>$$\int^{\infty}_{-\infty}g(x)dx = 1$$</p>
<blockquote>
<p>概率质量函数(the probability mass function)用于描述离散型随机变量; 概率密度函数(probability distribution function)用于描述连续型随机变量。</p>
</blockquote>
<p>累积分布函数(Cumulative Distribution Function/Probability distribution function):CDFs are monotonically increasing functions.It’s not strictly monotic though. There may be intervals of constancy.</p>
<p>$$pdf(x) = \frac{d}{dx}cdf(x)$$</p>
<blockquote>
<p>cdf是pdf在区间$[-\infty, \infty]$上的和，pdf(x)是cdf在「点」x上的斜率/导数</p>
</blockquote>
<hr>
<h3 id="Expected-Value-of-the-Function-of-a-Random-Variable-Law-of-the-Unconscious-Statistician"><a href="#Expected-Value-of-the-Function-of-a-Random-Variable-Law-of-the-Unconscious-Statistician" class="headerlink" title="Expected Value of the Function of a Random Variable: Law of the Unconscious Statistician"></a>Expected Value of the Function of a Random Variable: Law of the Unconscious Statistician</h3><p>统计师的无意识法则(law of the unconscious statistician):In practice, you don’t necessarily know the probability distribution of F(X). Of course you can calculate it, but this is an extra step, which you can avoid if you use the second method.</p>
<p>$$E[F(X)] = F[Y] = \sum F(X_i) P_X(X_i)$$</p>
<blockquote>
<p>假定函数$F(X)$是关于随机变量$X$的一个映射(因此$F(X)$本身也是随机变量)，那么举例而言$F(X) = (X - 3)^2$，现求函数$F(X)$的期望。</p>
<p>根据期望的定义(离散型随机变量$E[X] = \sum_{i=0}X_ipmf(X_i)$，连续型随机变量$E[X] = \int^{\infty}_{-\infty}Xpdf(X)$)，我们需要知道F(X)的$pdf(F(X))$。</p>
<p>若$X$的采样空间为$S = \{1,2,3,4,5,6\}$，那么对其所有的可能值进行$F(X)$计算</p>
</blockquote>
<p>$$\begin{array}{l}</p>
<p>X = 1, \; F(1) = (1-3)^2 = 4,\\X = 2, \; F(2) = (2-3)^2 = 1, \ X = 3,\;F(3) = (3-3)^2 = 0, \ X = 4,\;F(4) = (4-3)^2 = 1, \\X=5,\;F(5) = (5-3)^2 = 4,\ X = 6,\;F(6) = (6-3)^2 = 9.</p>
<p>\end{array}$$</p>
<blockquote>
<p>也就可以得到$pdf(F(X))$的「离散型表达」(连续性和离散型本质相同)</p>
</blockquote>
<p>$$\begin{array}{l} </p>
<p>Pr(F(0)) &amp;=&amp; \dfrac{1}{6}</p>
<p>\ Pr(F(1)) &amp;=&amp; \dfrac{1}{6} + \dfrac{1}{6} &amp;=&amp; \dfrac{2}{6} </p>
<p>\ Pr(F(4)) &amp;=&amp; \dfrac{1}{6} + \dfrac{1}{6} &amp;=&amp; \dfrac{2}{6}</p>
<p>\ Pr(F(9)) &amp;=&amp; \dfrac{1}{6}.</p>
<p>\end{array}$$</p>
<blockquote>
<p>最后一步计算$F(X)$的期望也就顺理成章</p>
</blockquote>
<p>$$\begin{split}</p>
<p>E[F(X)]&amp;=&amp;0 \times Pr(F(X) = 0) + 1 \times Pr(F(X) = 1)  + \\</p>
<p>&amp;&amp;4 \times Pr(F(X) = 4) + 9 \times Pr(F(X) = 9),\\</p>
<p>&amp;=&amp;0 <em> \dfrac{1}{6} + 1 </em> \dfrac{2}{6} + 4 <em> \dfrac{2}{6} + 9 </em> \dfrac {1}{6},\\</p>
<p>&amp;=&amp;3.167.</p>
<p>\end{split}$$</p>
<blockquote>
<p>所以这里使用随机变量$Y$代替随机变量$F(X)$，就有</p>
</blockquote>
<p>$$E[F(X)] = E[Y] = \sum Y_i P_Y(Y_i) …… ……<1>$$</1></p>
<blockquote>
<p>根本不需要知道$F(X)$的具体概率分布，直接根据随机变量$X$的概率分布就可以求出$Y$的期望。</p>
</blockquote>
<p>$$\begin{split}</p>
<p>E[F(X)] &amp; = &amp;(1-3)^2\times Pr(X = 1) + (2-3)^2\times Pr(X = 2) +\\</p>
<p>&amp;&amp; (3-3)^2\times Pr(X = 3)+ (4-3)^2\times Pr(X = 4) +\\</p>
<p>&amp;&amp; (5-3)^2\times Pr(X = 5) + (6-3)^2\times Pr(X = 6)\\</p>
<p>&amp;=&amp;4 <em> \dfrac{1}{6} + 1 </em> \dfrac{1}{6} + 0 <em> \dfrac{1}{6} + 1 </em> \dfrac{1}{6} + 4 <em> \dfrac{1}{6} + 4 </em> \dfrac{1}{6} \\</p>
<p>&amp;=&amp;3.167.</p>
<p>\end{split}$$</p>
<blockquote>
<p>原因在于，在计算期望的时候，多做了一步工作。也就是将上述式子当中相同的$F(X_i)$对应的概率进行了合并($Pr(F(X_i) = \sum_{j=0}{Pr(X_j)}(all  F(X_i) == F(X_j)))$)，但只为求解他的$pdf$。所以事实上这一步在求解期望过程中<strong>并不需要</strong>。</p>
</blockquote>
<p>$$E[F(X)] = E[Y] = \sum{F(X_i)P_X(X_i)}…… ……<2>$$</2></p>
<blockquote>
<p>对比$<1>$中，我们将中间合并过程得到的$Pr(Y_i)$直接替换为了$X$的概率分布。之所以称之为无意识的，是因为这一个过程非常的直观，以至于没有意识的状态下统计学家就得出了这样一个结论，同时也是毫无疑问正确的，因此而得名，<a href="https://www.quora.com/How-did-the-Law-of-the-Unconscious-Statistician-get-its-name" target="_blank" rel="noopener">How did the Law of the Unconscious Statistician get its name?</a></1></p>
</blockquote>
<hr>
<h3 id="The-Inverse-Transform-Sampling-Method"><a href="#The-Inverse-Transform-Sampling-Method" class="headerlink" title="The Inverse Transform Sampling Method"></a>The Inverse Transform Sampling Method</h3><p>简介背景，目前已有函数$PDF(X)$的一堆数据(这个$PDF(X)$可能是数学分析就可以解决的<a href="https://en.wikipedia.org/wiki/List_of_probability_distributions" target="_blank" rel="noopener">自然的概率分布</a>，或是任意的$PDF(X)$)，那么我们就可以通过求解$CDF(X)$的反函数$InvCDF(X)$，进行均匀采样计算就可以得到对应$PDF(X)$的密度分布。</p>
<p>1.计算机中求解的步骤是如何实现的?</p>
<p>首先对已有数据进行累加求解对应累积分布函数(CDF)，如图</p>
<figure><br><br><strong><br><a href="https://farm2.staticflickr.com/1544/24560765261_181ac9f907.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1544/24560765261_181ac9f907.jpg" alt=""><br></a>
</strong><br><br></figure>



<p>在y轴上进行均匀采样(Uniform Distribution)，假定当前生成随机数$r = 0.491$，那么求取其下界(最接近该y值的采样点，且采样点的$y_{sample} \leqslant y$)</p>
<figure><br><br><strong><br><a href="https://farm2.staticflickr.com/1711/24347461370_f19ac111f2.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1711/24347461370_f19ac111f2.jpg" alt=""><br></a>
</strong><br><br></figure>

<blockquote>
<p>以下证明方法和原文略有不同，更好理解</p>
</blockquote>
<p>令图中采样点$n = 15, n = 16$坐标为$(x_1, y_1), (x_2, y_2)$</p>
<figure><br><br><strong><br><a href="https://farm2.staticflickr.com/1595/24014884244_f53b11ae22.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1595/24014884244_f53b11ae22.jpg" alt=""><br></a>
</strong><br><br></figure>

<p>那么所求随机数生成点$(?, r)$的x轴坐标，令$dx$为采样点间间距，随机采样点x坐标「?」与下界$x_1$之差为k，其中采样区间为$[min, max]$，采样数量为$nSamples$</p>
<p>$$dx = \frac{min - max}{nSamples}$$</p>
<p>$$\frac{k}{dx} = \frac{r-y_1}{y_2-y_1} = t$$</p>
<p>$$k = t * dx$$</p>
<p>那么所求「?」即为</p>
<p>$$? = min + n_{lower} <em> dx + k = min + (n_{lower} + t)</em>dx$$</p>
<p>为了将「?」控制在区间$[0, 1]$之间($pdf(X)$定义)，需要做一次映射</p>
<p>$$? \backsim [min, max] \ x \backsim [0, 1]$$</p>
<p>$$\frac{x-min}{?-0} = \frac{max-x}{1-?} \\</p>
<p>? = \frac{x-min}{max-min}</p>
<p>$$</p>
<p>最终，得到横坐标值之后(也就是在$invCDF(X)$的x轴上均匀采样得到y值)绘图得到</p>
<figure><br><br><strong><br><a href="https://farm2.staticflickr.com/1691/24029017544_b3c7de509c.jpg" target="_blank" rel="noopener"><br>    <img src="https://farm2.staticflickr.com/1691/24029017544_b3c7de509c.jpg" alt=""><br></a>
</strong><br><br></figure>

<p>2.上述过程中并没有求反函数过程，为何要求反函数？</p>
<p>本质上当对$CDF(X)$的y轴进行均匀采样求解x的时候，就已经是在隐含求解了，不过因为不是所有的$CDF(X)$都可以显式的通过数学分析变换，因此计算机中求解使用的是通用过程。</p>
<p>3.指数分布的反函数求解过程</p>
<p>$$PDF(X) = \lambda e^{-\lambda x}$$</p>
<p>$$\begin{array}{l}</p>
<p>P(X&gt;t) &amp; = &amp; \int^{\infty}_t \lambda e^{-\lambda x}dx \\</p>
<p>&amp; = &amp; \int_{-\infty}^t \lambda e^{-\lambda x} -\frac{1}{\lambda}d(-\lambda x)</p>
<p>\end{array}$$</p>
<p>令$u = -\lambda x$，当$x = t$时，$u = - \lambda t$；当$x = \infty$, $u = -\infty$，变换函数$x = -\frac{1}{\lambda}u$在$[-\infty, -\lambda t]$上单值，$\frac{dx}{du} = -\frac{1}{\lambda}$在$[-\infty, -\lambda t]$上连续</p>
<p>$$\begin{array}{l}</p>
<p>P(X&gt;t) &amp; = &amp; -\int^{-\lambda t}_{-\infty}e^udu \\</p>
<p>&amp; = &amp; -\left[ e^u \right]^{-\lambda t}_{-\infty} \\</p>
<p>&amp; = &amp; e^{- \lambda t} - e^{-\infty} \\</p>
<p>&amp; = &amp; e^{- \lambda t}</p>
<p>\end{array}$$</p>
<p>那么求取$P(x&lt;t) = 1-e^{-\lambda x} = y$的反函数</p>
<p>$$\begin{array}{l}</p>
<p>y = 1-e^{-\lambda x} \\</p>
<p>e^{-\lambda x} = 1 - y \\</p>
<p>x = -\frac{1}{\lambda}ln(1-y)</p>
<p>\end{array}$$</p>
<p>那么最终对此函数进行采样，得到的结果就是指数分布。</p>
<blockquote>
<p>原文中有误，缺$dx$且最后结果e幂上少符号缺$\lambda$，y的表达式有误，目前已反馈给<a href="http://www.scratchapixel.com/" target="_blank" rel="noopener">Scratchapixel</a></p>
</blockquote>
<hr>
<h3 id="Estimators"><a href="#Estimators" class="headerlink" title="Estimators"></a>Estimators</h3><p>参数(Parameter):参数是指描述总体特征的一个或若干个数值，例如总体(Population)的均值、总体的比例和总体的方差等数字特征，两个或两个以上总体间的相关系数、偏相关系数、复相关系数和回归系数等数字特征。</p>
<blockquote>
<p>在一般情况下，总体参数是未知的，例如一个国家或地区的人口总数、GDP总量、小麦总产量、人均可支配收入和产品的合格率等都是总体未知参数，而通过全面调查取得这些未知参数需要付出高昂的成本。参数估计的目的就是利用抽样得到的样本信息来估计未知的总体参数(详情可见<a href="http://netclass.csu.edu.cn/NCourse/hep120/kj/6-1-1.htm" target="_blank" rel="noopener">参数的概念</a>)</p>
</blockquote>
<p>估计量(Estimator)与估计值(Estimate): The sample mean is a form of estimator, but in the general sense, an estimator is a function operating on observable data and returning an estimate of the population’s parameter value $θ$.</p>
<p>This function $δ$ is what we call an estimator of the parameter $θ$ and the result of $δ(x_1,…,x_n)$ is called an estimate of $θ$.(An estimation of the population’s paramter $θ$).</p>
<blockquote>
<p>采样均值其实就是总体未知参数的一个估计量，本质上估计量就是一组数据的函数。估计量就是随机变量$X_1, . . . , X_n$的一个映射，因此本身他也是随机变量。</p>
</blockquote>
<p>常见估计量</p>
<ol>
<li><strong>样本均值</strong>$\bar{X} = \frac{1}{n} \sum^n_{i=1}X_i$，是总体均值$E[X] = \mu$的估计量</li>
<li><strong>样本方差</strong>$S^2 = \frac{1}{n-1}\sum^n_{i=1}(X_i - \bar{X})^2$是总体方差$D(X)=\sigma^2$的估计量；<strong>样本标准差</strong>$S = \sqrt{\frac{1}{n-1}\sum^n_{i=1}(X_i - \bar{X})^2}$是总体标准差$\sigma$的估计量</li>
<li><strong>样本比例</strong>$\bar{p}=\frac{n_1}{n}$是总体比例p的估计量，其中$n_1$为样本中具有某种特征的样本单位数。</li>
</ol>
<p><br></p>
<p>估计量和估计值之间的区别: An estimate is a specific value $δ(x_1,…,x_n)$ of the estimator which we can determine by using observable values $x_1,…,x_n$. The estimator is a function $δ(X)$ of the random vector $X$ while again, an estimate is a just specific value $δ(x)$.</p>
<blockquote>
<p>一句话概括，估计值只是估计总体未知参数的某一估计量，代入样本值计算得到的具体结果</p>
<p><a href="http://netclass.csu.edu.cn/NCourse/hep120/kj/6-1-1.htm" target="_blank" rel="noopener">点估计和区间估计</a>，这里不做延伸阅读，简单的介绍了<strong>置信区间</strong>，<strong>置信度/置信概率/置信系数/置信水平</strong>，<strong>置信上下限</strong>等基本概念。</p>
</blockquote>
<hr>
<h3 id="Properties-of-Estimators"><a href="#Properties-of-Estimators" class="headerlink" title="Properties of Estimators"></a>Properties of Estimators</h3><p>无偏性(Unbias):</p>
<p>当采样的数量趋于极限时，样本均值就等于总体均值本身</p>
<p>$$\bar X_n \xrightarrow{p} \theta \quad \text{ for } n \rightarrow \infty.$$</p>
<p>也就是样本均值的期望就有着如下的关系</p>
<p>$$E[\bar X_n] - \theta = 0.$$</p>
<p>而满足无偏估计性质的样本均值，样本均值就是估计量本身，替换$\bar{X_n}$为$\sigma(X)$，就有以下两种情况了</p>
<p>$$E[\delta_{unbiased}(X)] - \theta = 0.$$</p>
<p>$$E[\delta_{biased}(X)] - \theta \neq 0.$$</p>
<p>二者之差就是偏差本身了</p>
<p>$$E[\delta_{biases}(X)] - \theta = \text{ bias }.$$</p>
<blockquote>
<p>正如先前讲到的，计算机图形学中经常采用有偏的方法来完成计算，原因在于有偏的方法可以带来更快速度的收敛计算(前提是满足一致性，这比无偏的性质对一个估计量而言更重要)，但却只与真实值之间有着微乎其微的误差。</p>
</blockquote>
<p>一致性(Consistency):是指随着样本容量的增大，估计量愈来愈接近总体参数的真值。设总体参数为$\theta$，$\delta$为一估计量，如果当样本容量$n \to \infty$时，  依概率收敛于$\theta$，即</p>
<p>$$P - lim_{n \to \infty}\delta = \theta$$</p>
<blockquote>
<p>如果一个估计量是一致估计量，那么可以通过增加样本容量来提高估计的精度和可靠性。</p>
<p>可以证明，<strong>样本均值</strong>$\bar{X}$是总体均值$\mu$的一致估计量；<strong>样本比例</strong>$\bar{p}$是总体比例$p$的一致估计量；<strong>样本方差</strong>$S^2$是总体方差$\sigma^2$的一致估计量；<strong>样本标准差</strong>$S$ 是总体标准差$\sigma$的一致估计量.(详情见<a href="http://netclass.csu.edu.cn/NCourse/hep120/kj/6-1-2.htm" target="_blank" rel="noopener">估计量评价的标准</a>)</p>
</blockquote>
<p>有效性(Variance):有效性是指估计量与总体参数的离散程度。如果两个估计量都是无偏的，那么离散程度较小的估计量相对而言是较为有效的。离散程度是用方差度量的，因此在无偏估计量中，方差愈小愈有效。</p>
<p>设$\theta_1$与$\theta_2$为总体参数$\theta$的无偏估计量，即$E(\theta_1)=\theta$，$E(\theta_2)=\theta$，那么如果两者的方差对比</p>
<p>$$D(\theta_1) \leqslant D(\theta_2)$$</p>
<p>那么称$\theta_1$会比$\theta_2$有效</p>
<blockquote>
<p>有效性是一个对比性质，因此是相对的，不存在绝对的自身有效的估计量。</p>
</blockquote>

                      </p>
                    </div>
                  </div>
                  <br>
                  <br>
                  
                  <div class="card is-white is-hover" id="articleCard">
                    <div class="card-header">
                      <a href="/2018/03/12/2016-1-25-Mathematical Foundations of Monte Carlo Methods 2/" class="card-header-title">
                        Mathematical Foundations of Monte Carlo Methods 2
                      </a>
                    </div>
                    <div class="card-content">
                      <p id="articleContent">
                        <blockquote>
<p>本文为在<a href="http://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-mathematical-foundations/quick-introduction-to-monte-carlo-methods" target="_blank" rel="noopener">Scratchapixel</a>上学习的翻译读后感与部分个人解读。这里不会将全篇的内容系数翻译，保留原文以便后期自行理解，笔者只精炼一些文章中关键的点出来便于记录。</p>
</blockquote>
<h3 id="Variance-and-Standard-Deviation"><a href="#Variance-and-Standard-Deviation" class="headerlink" title="Variance and Standard Deviation"></a>Variance and Standard Deviation</h3><p>方差与标准差(Variance and Standard Deviation):Standard deviation is simply the square root of variance, and variance is defined as the expected value of the square difference between the outcome of the experiment.</p>
<p>$$Var(X) = \sigma^2 = E[(X - E[X])^2] = \sum_i (x_i - E[X])^2p_i.$$</p>
<p>$$\text{Standard Deviation} = \sqrt{\sigma^2}.$$</p>
<blockquote>
<p>如若注意到的话，可以看到方差的符号标示上方有平方，这是为了避免潜在的符号干扰，本质上声明了方差和标准差是不可能为负数的。</p>
</blockquote>
<p>由于期望的可加性性质(上一篇笔记中有推导过程)，令$\mu = E[X]$，若随机变量$X$为常数，那么$E[c] = c$。</p>
<p>$$<br>\begin{array}{l}<br>E[X - E[X]^2] &amp; = &amp; E[(X - \mu)^2] \\<br>&amp; = &amp; E[X^2 - 2 \mu X + \mu^2] \\<br>&amp; = &amp; E[X^2] - 2 \mu E[X] + E[\mu^2] \\<br>&amp; = &amp; E[X^2] - 2\mu^2 + \mu^2 \\<br>&amp; = &amp; E[X^2] - \mu^2 \\<br>&amp; = &amp; \sum_i x_i^2 p_i - \mu^2 \\<br>&amp; = &amp; \sum_i x_i^2 p_i - (\sum_i x_i p_i)^2<br>\end{array}<br>$$</p>
<p>若当前随机变量表示的是一个等概率随机事件，那么方差可以直接根据其样本均值$\bar{X} = E[X]$构建计算</p>
<p>$$<br>\begin{array}{l}<br>Var(X) &amp; = &amp; \sum_i(x_i - E[X])^2 p_i\\<br>&amp; = &amp; \sum_{i=1}^n \frac{(x_i - \bar{X})^2}{n}<br>\end{array}<br>$$</p>
<hr>
<h3 id="Properies-of-Variance"><a href="#Properies-of-Variance" class="headerlink" title="Properies of Variance"></a>Properies of Variance</h3><ol>
<li><p>若$Pr(X = c) = 1$，那么其方差$Var(X) = \sum_i x_i^2p_i - \mu^2 = c^2*1 - c^2 =   0$。换言之就是，一个必然事件的方差为0。</p>
</li>
<li><p>若有事件$Y = aX + b$，那么其方差</p>
<p>$$<br>\begin{array}{l}<br>Var(Y) &amp; = &amp; E[(Y - E[Y])^2] \\<br>&amp; = &amp; E[(aX + b - E[aX + b])^2] \\<br>&amp; = &amp; E[(aX + b - aE[X] - b)^2] \\<br>&amp; = &amp; a^2 E[(X - E[X])^2] \\<br>&amp; = &amp; a^2Var(X)<br>\end{array}<br>$$</p>
</li>
<li><p>若$X_1, …, X_n$为独立随机变量，那么其方差$Var(X_1 + … + X_n) = Var(X_1) + … + Var(X_n).$</p>
</li>
</ol>
<p>这里只推导两个随机变量之间的相加，多项式可递推。令$\mu_1 = E[X_1], \mu_2 = E[X_2]$</p>
<p>，而$E[X_1 + X_2] = E[X_1] + E[X_2] = \mu_1 + \mu_2$，因此:</p>
<p>$$<br>\begin{array}{l}<br>Var(X_1 + X_2) &amp; = &amp; E[(X_1 + X_2 - E[X_1 + X_2])^2] \\<br>&amp; = &amp; E[(X_1 + X_2 - \mu_1 - \mu_2)^2] \\<br>&amp; = &amp; E[(X_1 - \mu_1)^2] + E[(X_2 - \mu_2)^2] - E[2(X_1 - \mu_1)(X_2 - \mu_2)] \\<br>&amp; = &amp; E[(X_1 - \mu_1)^2] + E[(X_2 - \mu_2)^2] - 2(E[(X_1 - \mu_1)]*E[X_2 - \mu_2]) \\<br>&amp; = &amp; E[(X_1 - \mu_1)^2] + E[(X_2 - \mu_2)^2] - 2((\mu_1 - \mu_1)(\mu_2 - \mu_2)) \\<br>&amp; = &amp; E[(X_1 - \mu_1)^2] + E[(X_2 - \mu_2)^2] \\<br>&amp; = &amp; Var(X_1) + Var(X_2)<br>\end{array}<br>$$</p>
<hr>
<h3 id="Probability-Distribution-Part-2"><a href="#Probability-Distribution-Part-2" class="headerlink" title="Probability Distribution: Part 2"></a>Probability Distribution: Part 2</h3><p>正态分布(Normal Distribution):$p(x) = \mathcal{N}(\mu, \sigma) = {\dfrac{1}{\sigma \sqrt {2 \pi} } } e^{-{\dfrac{(x -\mu)^2}{2\sigma^2}}}.$</p>
<p>其中$\mu$代表正态分布的期望，$\sigma$代表正态分布的标准差，整个曲线根据$\mu$对称。</p>
<figure><br>    <a href="https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg" target="_blank" rel="noopener"><br>        <img src="https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg" alt=""><br>    </a><br></figure>

<p>见图所示</p>
<hr>
<h3 id="Sampling-Distribution"><a href="#Sampling-Distribution" class="headerlink" title="Sampling Distribution"></a>Sampling Distribution</h3><p>样本分布(Sample Distribution):Each sample on its own, is a random variable, but because now they represent the mean of certain number n of items in the population, we label them with the upper letter $X$. We can repeat this experiment $N$ times which gives as series of samples: $X_1,X_2,…X_N$. This collection of samples is what we call a sampling distribution. </p>
<p>样本均值的期望(Expected value of the distribution of mean):We can apply to samples or statistics the same method for computing a mean than the method we used to calculate the mean of random variables. </p>
<figure><br>    <a href="https://farm2.staticflickr.com/1523/24254472969_395973fb3a.jpg" target="_blank" rel="noopener"><br>        <img src="https://farm2.staticflickr.com/1523/24254472969_395973fb3a.jpg" alt=""><br>    </a><br></figure>

<blockquote>
<p>注意到样本分布和普通的集群分布的区别，样本分布中，假定每个样本对集群取三次观察值，由于观察值本身是随机的缘故，因此观察值就是一个随机变量$x$。那么这样的一个样本分布的样本大小为3，所以样本均值$\bar{X_1} = E[x] = \frac{\sum^n_{i=1}x_i}{n}$，样本方差$Var(\bar{X_1}) = \sum^n_{i=1}(x_i - \bar{X_1})$。</p>
<p>上文说到的样本均值的期望的计算，<strong>也就是将最基本的观察值事件求取均值作为随机变量的期望计算</strong>，是讲样本这个群作为一个随机变量$X$，那么重复这样在总群中做采样，可以得到一系列$X_1, X_2, … X_N$，此时样本均值的期望$\mu_{\bar{X}} = E[\bar{X}] = \frac{\sum^N_{i=1}\bar{X_i}}{N}$，样本均值的方差$Var(\bar{X_1}) = \frac{\sum^N_{i-1}(\bar{X_i} - \mu_{\bar{X}})^2}{N}$。</p>
<p>所以务必要明确原文当中<strong>Expected value of the distribution of mean</strong>的含义才可得以进一步的计算。</p>
</blockquote>
<p><br></p>
<p>中心极限定理(Central Limit Theorem, CLT): The mean of the sampling distribution of the mean $μ_\bar{X}$ equals the mean of the population $μ$ and that the standard error of the distribution of means $μ_\bar{X}$ is equal to the standard deviation of the population $σ$ divided by the square root of $n$. In addition, the sampling distribution of the mean will approach a normal distribution $N(\mu, {\frac{\sigma}{\sqrt{n}}})$. These relationships may be summarized as follows:</p>
<p>$$μ_\bar{X} = μ_{σ\bar{X}}=\frac{σ}{\sqrt{n}}$$</p>
<hr>
<h3 id="Properties-of-the-Sample-Mean"><a href="#Properties-of-the-Sample-Mean" class="headerlink" title="Properties of the Sample Mean"></a>Properties of the Sample Mean</h3><ol>
<li><p>$\bar X = \dfrac{1}{n} (X_1 + … + X_n)$</p>
</li>
<li><p>样本均值等于总体平均值</p>
<p>$$E[\bar X_n] = \dfrac{1}{n} \sum_{i=1}^n E[X_i] = \dfrac{1}{n} \cdot { n \mu } = \mu.$$</p>
</li>
<li><p>遵循与基本事件$x$一样的性质(样本均值的期望本身就是随机变量$x$的期望所计算而来的均值，2已经证明样本期望就是总体期望本身)</p>
<p>$$\begin{array}{l}</p>
<p>E[aX+b] = aE[X] + b\\</p>
<p>E[X_1 + … + X_n] = E[X_1] + … + E[X_n].</p>
<p>\end{array}$$</p>
</li>
<li><p>也是样本期望的定义</p>
<p>$$\begin{array}{l}</p>
<p>E[\bar X]&amp;=&amp;E[\dfrac{1}{n}(X_1 + … + X_n)]\\</p>
<p>&amp;=&amp;\dfrac{1}{n}E[X_1 + … + X_n]\\</p>
<p>&amp;=&amp;\dfrac{1}{n} \sum_{i=1}^N E[X_i].</p>
<p>\end{array}$$</p>
</li>
<li><p>样本方差</p>
<p>$$\begin{array}{l}</p>
<p>Var(\bar X_n)&amp;=&amp;\dfrac{1}{n^2} Var \left( \sum_{i=1}^n X_i \right) \\</p>
<p>&amp;=&amp;\dfrac{1}{n^2} \sum_{i=1}^n Var(X_i) = \dfrac{1}{n^2} \cdot n \sigma^2 = \dfrac{\sigma^2}{n}.</p>
<p>\end{array}$$</p>
</li>
</ol>
<p>6.正如之前方差的定义中讲述的以下性质样本方差也都继承</p>
<p>$$\begin{array}{l}</p>
<p>Var(aX + b) = a^2Var(X)\\Var(X_1+…+X_n) = Var(X_1) + … + Var(X_n).</p>
<p>\end{array}$$</p>
<p>因此样本方差$\sigma^2$为:</p>
<p>$$\begin{array}{l}</p>
<p>Var(\bar X)&amp;=&amp;Var(\dfrac{1}{n}(X_1 + … X_n))\\</p>
<p>&amp;=&amp;\dfrac{1}{n^2 } Var(X_1 + … X_n)\\</p>
<p>&amp;=&amp;\dfrac{1}{n^2 } \sum_{i=1}^n Var(X_i).</p>
<p>\end{array}$$</p>
<p>7.因为样本方差为$\frac{\sigma^2}{n}$比总体方差$\sigma^2$要更小的关系(换言之样本标准差$\frac{\sigma}{\sqrt{n}}$)，样本均值$\bar{X}$会比单一观察量$X_i$所计算得到的期望$\mu$更接近</p>

                      </p>
                    </div>
                  </div>
                  <br>
                  <br>
                  
              
          </div>
        </div>
      </div>
      <!-- about me -->
      <div class="column">
        <div class="card is-white is-hover is-hidden-mobile is-sticky">
          <div class="card-image">
            <figure class="image">
              <img src="http://pbrt.org/gallery/a3.jpg">
            </figure>
          </div>
          <div class="card-content">
            <div class="media">
              <div class="media-left">
                <figure class="image is-48x48">
                  <img src="https://github.com/BentleyBlanks.png">
                </figure>
              </div>
              <div class="media-content">
                <p class="title is-4">Bingo</p>
                <p class="subtitle is-6">@BentleyJobs</p>
              </div>
            </div>
            <div class="content">
              Graduated from <a href="https://www.jiangnan.edu.cn/">JNU</a>, interested in cg & game developing, once worked in <a href="https://www.aurogon.com/">Aurogon</a> to develope <a href="https://gjol.yy.com/">GuJianOL</a>. See <a href="">about</a> for more info.
            </div>
            <!-- icon link -->
            <div class="level is-mobile" id="iconLink">
              <!-- <div class="column"></div> -->

              <div class="level-item has-text-centered">
                <a class="fa fa-lg fa-map-marker" href="https://www.google.com/maps/place/Ningbo,+Zhejiang,+China/@29.8694844,121.2917787,10z/data=!3m1!4b1!4m5!3m4!1s0x344d6354630858f7:0x948723f846ccf173!8m2!3d29.868336!4d121.54399"></a>
              </div>
              <div class="level-item has-text-centered">
                <a class="fa fa-lg fa-github" href="https://github.com/BentleyBlanks/"></a>
              </div>
              <div class="level-item has-text-centered">
                <a class="fa fa-lg fa-envelope" href="mailto:bentleyjobs@gmail.com?subject=Hello%20again"></a>
              </div>
              <div class="level-item has-text-centered">
                <a class="fa fa-lg fa-weibo" href="https://weibo.com/2162074794"></a>
              </div>

              <!-- <div class="column"></div> -->
            </div>
          </div>
        </div>
      </div>
      <!-- <div class="column"></div> -->
    </div>
  </div>
</section>

<div class="pagination-bar">
    <ul class="pagination">
        
        <li class="pagination-prev">
            
                <a class="btn btn--default btn--small" href="/archives/2018/page/4/">
            
                <i class="fa fa-angle-left text-base icon-mr"></i>
                    <span>pagination.newer_posts</span>
            </a>
        </li>
        
        
        <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/archives/2018/page/6/">
                    <span>pagination.older_posts</span>
                <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
        </li>
        
        <li class="pagination-number">pagination.page pagination.of</li>
    </ul>
</div>



<!-- Footer -->
<footer class="footer is-medium" id="footerStyle">
  <div class="container">
    <nav class="level is-mobile">
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>

      <div class="level-item has-text-centered">
        <img src="https://yellowdi.github.io/Elenore/images/elenore-icon-outlined.png" alt="Elenore logo" width="50">
      </div>

      <div class="level-item has-text-centered">
        <img src="https://raw.githubusercontent.com/hexojs/logo/master/hexo-logo-avatar-transparent-background.png" alt="Hexo logo" width="50">
      </div>
      
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
      <div class="level-item has-text-centered"></div>
    </nav>

    <div class="columns has-text-centered">
      <div class="column">
        Theme Floekr developed By <a href="https://github.com/BentleyBlanks">Bingo</a>, powered by <a href="https://yellowdi.github.io/Elenore">Elenore</a> and <a href="https://hexo.io/">Hexo</a>
      </div>
    </div>

  </div>
</footer>


  <!-- scripts list from theme config.yml -->
  
    <script src="/js/floekr.js" language="javascript"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  

</body>
</html>
